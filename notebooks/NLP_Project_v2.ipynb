{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBv8NDlBFYqy"
      },
      "source": [
        "## 1: Enable TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRWrgZrTFV5I",
        "outputId": "e404e6a0-ca73-40f6-ba04-b8843eeec4bc"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.4.0\n",
            "Running on TPU  ['10.122.196.74:8470']\n",
            "WARNING:tensorflow:TPU system grpc://10.122.196.74:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.122.196.74:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.122.196.74:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.122.196.74:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA6XX6dhFE_a"
      },
      "source": [
        "## 2: Import required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oCWLKGWAC9M",
        "outputId": "f701567f-61aa-4cf1-89b2-37869d2501f5"
      },
      "source": [
        "pip install PyArabic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyArabic in /usr/local/lib/python3.6/dist-packages (0.6.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uFTpabr6Apg"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from time import sleep\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import tensorflow_hub as hub\n",
        "import re\n",
        "import pyarabic.araby as araby"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_DOmZALFkDe"
      },
      "source": [
        "## 3: Generating required datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vecscJLREyFd",
        "outputId": "19ee53dc-22fb-4496-86df-8519616dc7db"
      },
      "source": [
        "# read sport news dataset\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/datasets/news_sport_dataset.csv\", encoding='utf-8')\n",
        "print(dataset.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  ...                                            Article\n",
            "0           0  ...  \\nجدد المنتخب المصري فوزه على مضيفه منتخب إي س...\n",
            "1           1  ...  \\nرغم التقدم المريح الذي حققه ليفربول الإنجليز...\n",
            "2           2  ...  \\nحقق هيوستن روكتس رقما قياسيا جديدا للنادي، ا...\n",
            "3           3  ...  \\nدبي، الإمارات العربية المتحدة (CNN)-- أعلن ا...\n",
            "4           4  ...  \\nأبدى المدافع الكرواتي فيدران تشورلوكا، الأرب...\n",
            "\n",
            "[5 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28rciwRXGBAt"
      },
      "source": [
        "# prepare sentences \n",
        "def sent_preparation(sentence):\n",
        "  # remove tashkeel\n",
        "  sentence = araby.strip_tashkeel(sentence)\n",
        "  # remove unnecessary char\n",
        "  sentence = sentence.replace(\"*\",\"\")\n",
        "  # remove newline char\n",
        "  sentence = sentence.replace(\"\\n\",\"\")\n",
        "  # remove repeated hyphens\n",
        "  regex = re.compile(r\"/(.)\\1{3,}/\")\n",
        "  sentence = regex.sub(\"\", sentence)\n",
        "  # remove hyphens\n",
        "  sentence = sentence.replace(\"- - - -\",\"\")\n",
        "  sentence = sentence.replace(\"- - -\",\"\")\n",
        "  sentence = sentence.replace(\"- -\",\"\")\n",
        "  #remove repeted dots\n",
        "  reg_r_dots = re.compile(r'\\.{2,}')\n",
        "  sentence = reg_r_dots.sub('', sentence)\n",
        "  # remove double quotation\n",
        "  sentence = sentence.replace('\"','')\n",
        "  # covert any english coma to arabic sample\n",
        "  sentence = sentence.replace(\",\", \"،\")\n",
        "  # add space before punctuation marks\n",
        "  sentence = re.sub('([-.,!?؟،:()])', r' \\1 ', sentence)\n",
        "  sentence = re.sub('\\s{2,}', ' ', sentence)\n",
        "  sentence = ' '.join(sentence.split())\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "QVShecyDGIKj",
        "outputId": "e7e68fd2-b0c7-44c3-8d4a-3478ec4f014f"
      },
      "source": [
        "# prepare article text\n",
        "dataset['Article'] = dataset['Article'].map(lambda sentence: sent_preparation(sentence))\n",
        "# show sample\n",
        "print(dataset['Article'].iloc[0])\n",
        "# get all articles content\n",
        "content = dataset['Article'].values\n",
        "content = \" \".join(content)\n",
        "content[:400]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "جدد المنتخب المصري فوزه على مضيفه منتخب إي سواتيني ، 2 - صفر ، خلال الجولة الرابعة من تصفيات كأس أمم أفريقيا ، على استاد السلام بالقاهرة ، الثلاثاء . وحملت ثنائية الفراعنة توقيع أحمد حجازي ( 19 ) مروان محسن ( 53 ) . وبهذه النتيجة رفع منتخب الفراعنة رصيده إلى 9 نقاط في المجموعة 11 من التصفيات ، والتي تضم إلى جانبه كلا من تونس وإي سواتيني والنيجر . وكانت مباراة الذهاب قد انتهت بفوز رفاق محمد صلاح على إي سواتيني 4 - 1 . وشهدت المباراة غياب نجم ليفربول محمد صلاح ، بداعي إصابة تعرض لها في مباراة الذهاب . وبخسارتها الثالثة في أربع جولات مقابل تعادل واحد ، أصبحت إي سواتيني خارج حسابات التأهل إلى النهائيات التي يبلغها بطل ووصيف المجموعة ، وفي حال تعادل تونس أو فوزها لاحقا على النيجر ( نقطة واحدة ) ، ستضمن مع مصر بلوغها النهائيات ، قبل جولتين على نهاية التصفيات . \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'جدد المنتخب المصري فوزه على مضيفه منتخب إي سواتيني ، 2 - صفر ، خلال الجولة الرابعة من تصفيات كأس أمم أفريقيا ، على استاد السلام بالقاهرة ، الثلاثاء . وحملت ثنائية الفراعنة توقيع أحمد حجازي ( 19 ) مروان محسن ( 53 ) . وبهذه النتيجة رفع منتخب الفراعنة رصيده إلى 9 نقاط في المجموعة 11 من التصفيات ، والتي تضم إلى جانبه كلا من تونس وإي سواتيني والنيجر . وكانت مباراة الذهاب قد انتهت بفوز رفاق محمد صلاح عل'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C6IsVVtGU5c"
      },
      "source": [
        "# split articles to a fixed sequence length\n",
        "def splitTextToSequences(string, lenght=15):\n",
        "    words = string.split()\n",
        "    grouped_words = [' '.join(words[i: i + lenght]) for i in range(0, len(words), lenght)]\n",
        "    return grouped_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caHgyWNMGwhP",
        "outputId": "4d3f39fc-39ed-4f05-a2b1-25d6b5914960"
      },
      "source": [
        "lines = splitTextToSequences(content) \n",
        "print('Number of sequnces: ', len(lines))\n",
        "print(\"Samle ========================>\")\n",
        "print(lines[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequnces:  107714\n",
            "Samle ========================>\n",
            "جدد المنتخب المصري فوزه على مضيفه منتخب إي سواتيني ، 2 - صفر ، خلال\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_z8zRZlG2r0"
      },
      "source": [
        "# mask sentence words\n",
        "def mask (sentence):\n",
        "  sentence = sentence.split()\n",
        "  if len(sentence) > 0:\n",
        "    punc_lsit = [\"؟\", \".\", \":\", \"!\", \",\", \"?\", \"،\"]\n",
        "    for id, word in enumerate(sentence):\n",
        "      if word in punc_lsit:\n",
        "        continue\n",
        "      sentence[id] = 'space'\n",
        "    return \" \".join(sentence)\n",
        "\n",
        "# remove punctionation marks\n",
        "def remove_punctionation(sentence):\n",
        "  return \"\".join(u for u in sentence if u not in (\"؟\", \".\", \":\", \"!\", \",\", \"?\", \"،\"))\n",
        "\n",
        "# check if sentence have punc\n",
        "def has_punctuations(sentence):\n",
        "  punc_lsit = [\"؟\", \".\", \":\", \"!\", \",\", \"?\", \"،\"]\n",
        "  # checking if string contains list element \n",
        "  res = [ele for ele in punc_lsit if(ele in sentence)]\n",
        "  return bool(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByEtVju8HAPD",
        "outputId": "3f951fad-152d-428b-c319-a6f3c173627e"
      },
      "source": [
        "# remove all sequences that not have any punctuation marks\n",
        "lines_filtered = filter(lambda x: has_punctuations(x) == True, lines)\n",
        "\n",
        "# creating source dataset \n",
        "source_df = pd.DataFrame(lines_filtered, columns= ['sentences'])\n",
        "source_df['sentences']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        جدد المنتخب المصري فوزه على مضيفه منتخب إي سوا...\n",
              "1        الجولة الرابعة من تصفيات كأس أمم أفريقيا ، على...\n",
              "2        وحملت ثنائية الفراعنة توقيع أحمد حجازي ( 19 ) ...\n",
              "3        وبهذه النتيجة رفع منتخب الفراعنة رصيده إلى 9 ن...\n",
              "4        والتي تضم إلى جانبه كلا من تونس وإي سواتيني وا...\n",
              "                               ...                        \n",
              "91870    وقال النفيعي لـفي المرمى : سنحل مشكلة الديون خ...\n",
              "91871    تكليفي من تركي آل الشيخ رئيس مجلس إدارة الهيئة...\n",
              "91872    نيل رضا جمهور الأهلي يحتاج إلى تعب كبير ، وأسع...\n",
              "91873    ، وختم قائلا : سنجدد مع بعض اللاعبين الأجانب ،...\n",
              "91874    هوية اللاعبين الآخرين ، وسيصل المدرب الجديد خل...\n",
              "Name: sentences, Length: 91875, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQxBoKGXh6BY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c052cc1-dc83-41de-8f81-09bd9fc349f1"
      },
      "source": [
        "source_df['sentences'] = source_df['sentences'].map(lambda sentence: remove_punctionation(sentence))\n",
        "source_df['sentences'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'جدد المنتخب المصري فوزه على مضيفه منتخب إي سواتيني  2 - صفر  خلال'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs8oA3GvHG9d",
        "outputId": "9c39c5a3-a7b4-4808-b353-714d7838a354"
      },
      "source": [
        "print(source_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               sentences\n",
            "0      جدد المنتخب المصري فوزه على مضيفه منتخب إي سوا...\n",
            "1      الجولة الرابعة من تصفيات كأس أمم أفريقيا  على ...\n",
            "2      وحملت ثنائية الفراعنة توقيع أحمد حجازي ( 19 ) ...\n",
            "3      وبهذه النتيجة رفع منتخب الفراعنة رصيده إلى 9 ن...\n",
            "4      والتي تضم إلى جانبه كلا من تونس وإي سواتيني وا...\n",
            "...                                                  ...\n",
            "91870  وقال النفيعي لـفي المرمى  سنحل مشكلة الديون خل...\n",
            "91871  تكليفي من تركي آل الشيخ رئيس مجلس إدارة الهيئة...\n",
            "91872  نيل رضا جمهور الأهلي يحتاج إلى تعب كبير  وأسعى...\n",
            "91873   وختم قائلا  سنجدد مع بعض اللاعبين الأجانب  بي...\n",
            "91874  هوية اللاعبين الآخرين  وسيصل المدرب الجديد خلا...\n",
            "\n",
            "[91875 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr5Z1ZLTHL1G"
      },
      "source": [
        "# save source dataset\n",
        "source_df.to_csv ('/content/drive/MyDrive/datasets/source_sentence_dataset.csv', header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FjG4N5qNHhAZ",
        "outputId": "1a9c8f82-c14c-47ba-e9fd-f870111d5aa7"
      },
      "source": [
        "# remove all sequences that not have any punctuation marks\n",
        "lines_filtered = filter(lambda x: has_punctuations(x) == True, lines)\n",
        "\n",
        "# creating target dataset  \n",
        "target_df = pd.DataFrame(lines_filtered, columns= ['sentences'])\n",
        "target_df['sentences'] = target_df['sentences'].map(lambda sentence: mask(sentence))\n",
        "target_df['sentences'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'space space space space space space space space space ، space space space ، space'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0EUYsXAHkU6",
        "outputId": "35d48648-005e-4061-84c7-7df0e8c719e0"
      },
      "source": [
        "print(target_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               sentences\n",
            "0      space space space space space space space spac...\n",
            "1      space space space space space space space ، sp...\n",
            "2      space space space space space space space spac...\n",
            "3      space space space space space space space spac...\n",
            "4      space space space space space space space spac...\n",
            "...                                                  ...\n",
            "91870  space space space space : space space space sp...\n",
            "91871  space space space space space space space spac...\n",
            "91872  space space space space space space space spac...\n",
            "91873  ، space space : space space space space space ...\n",
            "91874  space space space ، space space space space sp...\n",
            "\n",
            "[91875 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ty6U3H0HpEa"
      },
      "source": [
        "# save source target\n",
        "target_df.to_csv ('/content/drive/MyDrive/datasets/target_sentence_dataset.csv', header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGIHElQsH254"
      },
      "source": [
        "## 4: Preparing Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-o6RWesHtz3"
      },
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.replace(\"\\n\",\"\")\n",
        "    sentence = ' '.join(sentence.split())\n",
        "    text = '<start> '+ sentence + ' <end>'\n",
        "    return text\n",
        "\n",
        "# calculates the max length in tensor\n",
        "def calculate_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "# convert inputs to numeric sequences with the maximum length\n",
        "def tokenize(text):\n",
        "    # Choose the top 15000 words from the vocabulary\n",
        "    top_k = 15000\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, oov_token=\"<unk>\",filters='')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    train_seqs = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "    #add a word for padding \n",
        "    tokenizer.word_index['<pad>'] = 0\n",
        "    tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "    # Create the tokenized vectors\n",
        "    text_seqs = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "    # Pad each vector to the max_length of the vector\n",
        "    text_vector = tf.keras.preprocessing.sequence.pad_sequences(text_seqs, padding='post')\n",
        "\n",
        "    # Calculates the max_length, which is used to store the attention weights\n",
        "    max_length = calculate_max_length(text_seqs)\n",
        "    \n",
        "    return tokenizer, text_vector, max_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "pDQfRK16H_mN",
        "outputId": "f5ee2212-cce8-4244-87e0-4419e1129760"
      },
      "source": [
        "# load source dataset\n",
        "source_dataset = pd.read_csv(\"/content/drive/MyDrive/datasets/source_sentence_dataset.csv\", encoding='utf-8')\n",
        "#source_dataset.drop(source_dataset.tail(1).index,inplace=True)\n",
        "source_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>جدد المنتخب المصري فوزه على مضيفه منتخب إي سوا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>الجولة الرابعة من تصفيات كأس أمم أفريقيا  على ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>وحملت ثنائية الفراعنة توقيع أحمد حجازي ( 19 ) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>وبهذه النتيجة رفع منتخب الفراعنة رصيده إلى 9 ن...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>والتي تضم إلى جانبه كلا من تونس وإي سواتيني وا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91870</th>\n",
              "      <td>91870</td>\n",
              "      <td>وقال النفيعي لـفي المرمى  سنحل مشكلة الديون خل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91871</th>\n",
              "      <td>91871</td>\n",
              "      <td>تكليفي من تركي آل الشيخ رئيس مجلس إدارة الهيئة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91872</th>\n",
              "      <td>91872</td>\n",
              "      <td>نيل رضا جمهور الأهلي يحتاج إلى تعب كبير  وأسعى...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91873</th>\n",
              "      <td>91873</td>\n",
              "      <td>وختم قائلا  سنجدد مع بعض اللاعبين الأجانب  بي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91874</th>\n",
              "      <td>91874</td>\n",
              "      <td>هوية اللاعبين الآخرين  وسيصل المدرب الجديد خلا...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>91875 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                                          sentences\n",
              "0               0  جدد المنتخب المصري فوزه على مضيفه منتخب إي سوا...\n",
              "1               1  الجولة الرابعة من تصفيات كأس أمم أفريقيا  على ...\n",
              "2               2  وحملت ثنائية الفراعنة توقيع أحمد حجازي ( 19 ) ...\n",
              "3               3  وبهذه النتيجة رفع منتخب الفراعنة رصيده إلى 9 ن...\n",
              "4               4  والتي تضم إلى جانبه كلا من تونس وإي سواتيني وا...\n",
              "...           ...                                                ...\n",
              "91870       91870  وقال النفيعي لـفي المرمى  سنحل مشكلة الديون خل...\n",
              "91871       91871  تكليفي من تركي آل الشيخ رئيس مجلس إدارة الهيئة...\n",
              "91872       91872  نيل رضا جمهور الأهلي يحتاج إلى تعب كبير  وأسعى...\n",
              "91873       91873   وختم قائلا  سنجدد مع بعض اللاعبين الأجانب  بي...\n",
              "91874       91874  هوية اللاعبين الآخرين  وسيصل المدرب الجديد خلا...\n",
              "\n",
              "[91875 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "j0uNO8Q2IKlP",
        "outputId": "fe0a09e4-6df5-415d-8799-cb1ae43a4e91"
      },
      "source": [
        "# load target dataset\n",
        "target_dataset = pd.read_csv(\"/content/drive/MyDrive/datasets/target_sentence_dataset.csv\", encoding='utf-8')\n",
        "#target_dataset.drop(target_dataset.tail(1).index,inplace=True)\n",
        "target_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>sentences</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>space space space space space space space spac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>space space space space space space space ، sp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>space space space space space space space spac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>space space space space space space space spac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>space space space space space space space spac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91870</th>\n",
              "      <td>91870</td>\n",
              "      <td>space space space space : space space space sp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91871</th>\n",
              "      <td>91871</td>\n",
              "      <td>space space space space space space space spac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91872</th>\n",
              "      <td>91872</td>\n",
              "      <td>space space space space space space space spac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91873</th>\n",
              "      <td>91873</td>\n",
              "      <td>، space space : space space space space space ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91874</th>\n",
              "      <td>91874</td>\n",
              "      <td>space space space ، space space space space sp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>91875 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0                                          sentences\n",
              "0               0  space space space space space space space spac...\n",
              "1               1  space space space space space space space ، sp...\n",
              "2               2  space space space space space space space spac...\n",
              "3               3  space space space space space space space spac...\n",
              "4               4  space space space space space space space spac...\n",
              "...           ...                                                ...\n",
              "91870       91870  space space space space : space space space sp...\n",
              "91871       91871  space space space space space space space spac...\n",
              "91872       91872  space space space space space space space spac...\n",
              "91873       91873  ، space space : space space space space space ...\n",
              "91874       91874  space space space ، space space space space sp...\n",
              "\n",
              "[91875 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "cZ5_BVIYITBn",
        "outputId": "fe851bba-f9ea-4b74-f0d1-1820cf9c5308"
      },
      "source": [
        "# prepare input data\n",
        "source_dataset['sentences'] = source_dataset['sentences'].map(lambda sentence: preprocess_sentence(sentence))\n",
        "input_tokenizer, input_tensor ,input_max_length = tokenize(source_dataset['sentences'].values)\n",
        "source_dataset['sentences']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-300431e4c1f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# prepare input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msource_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocess_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0minput_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0minput_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msource_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'source_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1RRQ1PiIYde",
        "outputId": "686803c6-9f43-4f21-e8aa-226c87325c97"
      },
      "source": [
        "# prepare output data\n",
        "target_dataset['sentences'] = target_dataset['sentences'].map(lambda sentence: preprocess_sentence(sentence))\n",
        "target_tokenizer, target_tensor ,target_max_length = tokenize(target_dataset['sentences'].values)\n",
        "target_dataset['sentences']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        <start> space space space space space space sp...\n",
              "1        <start> space space space space space space sp...\n",
              "2        <start> space space space space space space sp...\n",
              "3        <start> space space space space space space sp...\n",
              "4        <start> space space space space space space sp...\n",
              "                               ...                        \n",
              "91870    <start> space space space space : space space ...\n",
              "91871    <start> space space space space space space sp...\n",
              "91872    <start> space space space space space space sp...\n",
              "91873    <start> ، space space : space space space spac...\n",
              "91874    <start> space space space ، space space space ...\n",
              "Name: sentences, Length: 91875, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEBHjwznIc3Q",
        "outputId": "27e0aa7b-b2f8-4142-b199-a04a366dd7a4"
      },
      "source": [
        "# data shapes\n",
        "(input_tensor.shape),(target_tensor.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((91875, 16), (91875, 17))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kf2Xar9Ifv9",
        "outputId": "533fc51b-c64c-42d9-8bdf-b97a7d431274"
      },
      "source": [
        "# convert index to word for input tensor\n",
        "def convert(tokenizer, tensor):\n",
        "    '''\n",
        "    INPUT: \n",
        "    tokenizer: object of converted text into a sequence of integer\n",
        "    tensor: list of integer\n",
        "    ''' \n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "              print (\"%d ----> %s\" % (t, tokenizer.index_word[t]))\n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(input_tokenizer, input_tensor[0])  \n",
        "print (\"Output Language; index to word mapping\")\n",
        "convert(target_tokenizer, target_tensor[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "2 ----> <start>\n",
            "2921 ----> جدد\n",
            "19 ----> المنتخب\n",
            "60 ----> المصري\n",
            "429 ----> فوزه\n",
            "6 ----> على\n",
            "663 ----> مضيفه\n",
            "45 ----> منتخب\n",
            "2580 ----> إي\n",
            "8079 ----> سواتيني\n",
            "97 ----> 2\n",
            "14 ----> -\n",
            "170 ----> صفر\n",
            "37 ----> خلال\n",
            "3 ----> <end>\n",
            "Output Language; index to word mapping\n",
            "3 ----> <start>\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "5 ----> ،\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "2 ----> space\n",
            "5 ----> ،\n",
            "2 ----> space\n",
            "4 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aza6jy4ImYt"
      },
      "source": [
        "## 5: Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "zWsrut8OIot0",
        "outputId": "60c6eed0-8e63-4fa9-eaf7-16d45d41f23d"
      },
      "source": [
        "# overview of the numbers of punctuation marks in our dataset\n",
        "target_plot = pd.DataFrame(list(target_tokenizer.word_counts.items())) \n",
        "target_plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;start&gt;</td>\n",
              "      <td>91875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>space</td>\n",
              "      <td>1235129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>،</td>\n",
              "      <td>80350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;end&gt;</td>\n",
              "      <td>91875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>.</td>\n",
              "      <td>54432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>:</td>\n",
              "      <td>6655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>!</td>\n",
              "      <td>826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>؟</td>\n",
              "      <td>729</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1\n",
              "0  <start>    91875\n",
              "1    space  1235129\n",
              "2        ،    80350\n",
              "3    <end>    91875\n",
              "4        .    54432\n",
              "5        :     6655\n",
              "6        !      826\n",
              "7        ؟      729"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "VAg7pAGxIu4o",
        "outputId": "1917002e-f743-46ad-d5c9-e5955601320a"
      },
      "source": [
        "plt.xticks(fontsize =10)\n",
        "plt.bar(target_plot[0],target_plot[1], color=(0.2, 0.4, 0.6, 0.6))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 8 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYbM___pIyhD"
      },
      "source": [
        "## 6: Buliding the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI0b4m1GI0SP"
      },
      "source": [
        "# initialize the main variables\n",
        "BUFFER_SIZE = len(input_tensor)\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 1024\n",
        "vocab_inp_size = len(input_tokenizer.word_index)+1\n",
        "vocab_tar_size = len(target_tokenizer.word_index)+1\n",
        "\n",
        "# create dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSANIg1_I4j3",
        "outputId": "1841210b-2f3f-461d-a7d0-8e5cfc2c996b"
      },
      "source": [
        "# input / output batches\n",
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([128, 16]), TensorShape([128, 17]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPe6cXXSI7SU"
      },
      "source": [
        "# Encoder class implementation as Tensorflow guide\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpMRP3ZAI9v8"
      },
      "source": [
        "# Bahdanau Attention Layer implementation as Tensorflow guide\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.keras.layers.Activation(activation = \"tanh\")(tf.keras.layers.Add()([self.W1(values), self.W2(hidden_with_time_axis)])))    \n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.keras.layers.Activation(activation = \"softmax\")(tf.keras.layers.Permute((2, 1))(score))\n",
        "        attention_weights = tf.keras.layers.Permute((2, 1))(attention_weights)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = tf.keras.layers.Multiply()([attention_weights, values])\n",
        "\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B181UoD9JAzA"
      },
      "source": [
        "# Decoder class implementation as Tensorflow guide\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.keras.layers.Concatenate(axis = -1)([tf.expand_dims(context_vector, 1), x])\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        #output = tf.keras.layers.Dense(self.dec_units)(output)\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.keras.layers.Reshape((output.shape[2],))(output)\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkw-CmGEJEpV"
      },
      "source": [
        "# 1: initialize the encoder network\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "# sample output\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "# 2: initialize an attention layer\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "# 3: initialize the decoder network\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "# sample output\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                        sample_hidden, sample_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE8ihXvEJIH9"
      },
      "source": [
        "# Optimizer and loss function as as Tensorflow guide\n",
        "# Adam optimizer\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "# Sparse Categorical Crossentropy\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "# loos function, we will use it in traing step\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAsRq9gPJI99"
      },
      "source": [
        "# Checkpoints (Object-based saving)\n",
        "checkpoint_dir = '/content/drive/MyDrive/datasets/models/npl_project_check_v2/cp.ckpt'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
        "local_device_option = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SihAcFzSJcBh"
      },
      "source": [
        "## 7: Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a10Hl8e7Jebg"
      },
      "source": [
        "# train encode-decode model as Tensorflow guide\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "          # passing enc_output to the decoder\n",
        "          predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "          loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc7tWDd8JisS",
        "outputId": "6fc3a6ce-9abb-49d8-ba39-bcd6323bea1f"
      },
      "source": [
        "# start training\n",
        "EPOCHS = 60\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # epoch start time\n",
        "  start = time.time()\n",
        "  # init hidden state\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix, options=local_device_option)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.2578\n",
            "Epoch 1 Batch 100 Loss 0.5888\n",
            "Epoch 1 Batch 200 Loss 0.5610\n",
            "Epoch 1 Batch 300 Loss 0.5750\n",
            "Epoch 1 Batch 400 Loss 0.5184\n",
            "Epoch 1 Batch 500 Loss 0.4943\n",
            "Epoch 1 Batch 600 Loss 0.5546\n",
            "Epoch 1 Batch 700 Loss 0.5588\n",
            "Epoch 1 Loss 0.5605\n",
            "Time taken for 1 epoch 390.5585207939148 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5785\n",
            "Epoch 2 Batch 100 Loss 0.5376\n",
            "Epoch 2 Batch 200 Loss 0.5412\n",
            "Epoch 2 Batch 300 Loss 0.5720\n",
            "Epoch 2 Batch 400 Loss 0.5632\n",
            "Epoch 2 Batch 500 Loss 0.4984\n",
            "Epoch 2 Batch 600 Loss 0.5270\n",
            "Epoch 2 Batch 700 Loss 0.6250\n",
            "Epoch 2 Loss 0.5345\n",
            "Time taken for 1 epoch 368.66062235832214 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.5508\n",
            "Epoch 3 Batch 100 Loss 0.5699\n",
            "Epoch 3 Batch 200 Loss 0.5347\n",
            "Epoch 3 Batch 300 Loss 0.5391\n",
            "Epoch 3 Batch 400 Loss 0.5219\n",
            "Epoch 3 Batch 500 Loss 0.5775\n",
            "Epoch 3 Batch 600 Loss 0.5493\n",
            "Epoch 3 Batch 700 Loss 0.5024\n",
            "Epoch 3 Loss 0.5428\n",
            "Time taken for 1 epoch 365.18446373939514 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.4920\n",
            "Epoch 4 Batch 100 Loss 0.4696\n",
            "Epoch 4 Batch 200 Loss 0.5829\n",
            "Epoch 4 Batch 300 Loss 0.5687\n",
            "Epoch 4 Batch 400 Loss 0.5658\n",
            "Epoch 4 Batch 500 Loss 0.5332\n",
            "Epoch 4 Batch 600 Loss 0.5498\n",
            "Epoch 4 Batch 700 Loss 0.5622\n",
            "Epoch 4 Loss 0.5427\n",
            "Time taken for 1 epoch 369.17533230781555 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.5481\n",
            "Epoch 5 Batch 100 Loss 0.5142\n",
            "Epoch 5 Batch 200 Loss 0.5202\n",
            "Epoch 5 Batch 300 Loss 0.5405\n",
            "Epoch 5 Batch 400 Loss 0.5477\n",
            "Epoch 5 Batch 500 Loss 0.5627\n",
            "Epoch 5 Batch 600 Loss 0.5510\n",
            "Epoch 5 Batch 700 Loss 0.5455\n",
            "Epoch 5 Loss 0.5498\n",
            "Time taken for 1 epoch 367.0244140625 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.5594\n",
            "Epoch 6 Batch 100 Loss 0.5378\n",
            "Epoch 6 Batch 200 Loss 0.5431\n",
            "Epoch 6 Batch 300 Loss 0.6738\n",
            "Epoch 6 Batch 400 Loss 0.5705\n",
            "Epoch 6 Batch 500 Loss 0.5521\n",
            "Epoch 6 Batch 600 Loss 0.5617\n",
            "Epoch 6 Batch 700 Loss 0.5484\n",
            "Epoch 6 Loss 0.5488\n",
            "Time taken for 1 epoch 369.41511368751526 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.5363\n",
            "Epoch 7 Batch 100 Loss 0.5291\n",
            "Epoch 7 Batch 200 Loss 0.5410\n",
            "Epoch 7 Batch 300 Loss 0.5598\n",
            "Epoch 7 Batch 400 Loss 0.5504\n",
            "Epoch 7 Batch 500 Loss 0.5541\n",
            "Epoch 7 Batch 600 Loss 0.5324\n",
            "Epoch 7 Batch 700 Loss 0.5372\n",
            "Epoch 7 Loss 0.5437\n",
            "Time taken for 1 epoch 367.2569055557251 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5186\n",
            "Epoch 8 Batch 100 Loss 0.5385\n",
            "Epoch 8 Batch 200 Loss 0.5379\n",
            "Epoch 8 Batch 300 Loss 0.5219\n",
            "Epoch 8 Batch 400 Loss 0.5404\n",
            "Epoch 8 Batch 500 Loss 0.5412\n",
            "Epoch 8 Batch 600 Loss 0.5502\n",
            "Epoch 8 Batch 700 Loss 0.5302\n",
            "Epoch 8 Loss 0.5352\n",
            "Time taken for 1 epoch 369.8038628101349 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.5049\n",
            "Epoch 9 Batch 100 Loss 0.5120\n",
            "Epoch 9 Batch 200 Loss 0.5137\n",
            "Epoch 9 Batch 300 Loss 0.5288\n",
            "Epoch 9 Batch 400 Loss 0.5326\n",
            "Epoch 9 Batch 500 Loss 0.5201\n",
            "Epoch 9 Batch 600 Loss 0.5227\n",
            "Epoch 9 Batch 700 Loss 0.5277\n",
            "Epoch 9 Loss 0.5280\n",
            "Time taken for 1 epoch 367.3909227848053 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.5166\n",
            "Epoch 10 Batch 100 Loss 0.5363\n",
            "Epoch 10 Batch 200 Loss 0.5322\n",
            "Epoch 10 Batch 300 Loss 0.5283\n",
            "Epoch 10 Batch 400 Loss 0.5056\n",
            "Epoch 10 Batch 500 Loss 0.5268\n",
            "Epoch 10 Batch 600 Loss 0.5212\n",
            "Epoch 10 Batch 700 Loss 0.5220\n",
            "Epoch 10 Loss 0.5217\n",
            "Time taken for 1 epoch 370.105313539505 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.5070\n",
            "Epoch 11 Batch 100 Loss 0.5172\n",
            "Epoch 11 Batch 200 Loss 0.5160\n",
            "Epoch 11 Batch 300 Loss 0.5128\n",
            "Epoch 11 Batch 400 Loss 0.5259\n",
            "Epoch 11 Batch 500 Loss 0.5215\n",
            "Epoch 11 Batch 600 Loss 0.5126\n",
            "Epoch 11 Batch 700 Loss 0.5154\n",
            "Epoch 11 Loss 0.5146\n",
            "Time taken for 1 epoch 366.99083709716797 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.4892\n",
            "Epoch 12 Batch 100 Loss 0.5010\n",
            "Epoch 12 Batch 200 Loss 0.4946\n",
            "Epoch 12 Batch 300 Loss 0.5026\n",
            "Epoch 12 Batch 400 Loss 0.5056\n",
            "Epoch 12 Batch 500 Loss 0.5202\n",
            "Epoch 12 Batch 600 Loss 0.5081\n",
            "Epoch 12 Batch 700 Loss 0.5054\n",
            "Epoch 12 Loss 0.5079\n",
            "Time taken for 1 epoch 368.728404045105 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.5116\n",
            "Epoch 13 Batch 100 Loss 0.4928\n",
            "Epoch 13 Batch 200 Loss 0.5059\n",
            "Epoch 13 Batch 300 Loss 0.5006\n",
            "Epoch 13 Batch 400 Loss 0.4850\n",
            "Epoch 13 Batch 500 Loss 0.5097\n",
            "Epoch 13 Batch 600 Loss 0.4956\n",
            "Epoch 13 Batch 700 Loss 0.4833\n",
            "Epoch 13 Loss 0.4935\n",
            "Time taken for 1 epoch 365.42830085754395 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.4753\n",
            "Epoch 14 Batch 100 Loss 0.4660\n",
            "Epoch 14 Batch 200 Loss 0.5347\n",
            "Epoch 14 Batch 300 Loss 0.4984\n",
            "Epoch 14 Batch 400 Loss 0.4875\n",
            "Epoch 14 Batch 500 Loss 0.4909\n",
            "Epoch 14 Batch 600 Loss 0.4823\n",
            "Epoch 14 Batch 700 Loss 0.4846\n",
            "Epoch 14 Loss 0.4851\n",
            "Time taken for 1 epoch 368.41333293914795 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.4664\n",
            "Epoch 15 Batch 100 Loss 0.4541\n",
            "Epoch 15 Batch 200 Loss 0.4611\n",
            "Epoch 15 Batch 300 Loss 0.4548\n",
            "Epoch 15 Batch 400 Loss 0.4241\n",
            "Epoch 15 Batch 500 Loss 0.4813\n",
            "Epoch 15 Batch 600 Loss 0.5514\n",
            "Epoch 15 Batch 700 Loss 0.4487\n",
            "Epoch 15 Loss 0.4604\n",
            "Time taken for 1 epoch 366.0189859867096 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.4386\n",
            "Epoch 16 Batch 100 Loss 0.4323\n",
            "Epoch 16 Batch 200 Loss 0.4712\n",
            "Epoch 16 Batch 300 Loss 0.4476\n",
            "Epoch 16 Batch 400 Loss 0.4456\n",
            "Epoch 16 Batch 500 Loss 0.4517\n",
            "Epoch 16 Batch 600 Loss 0.4165\n",
            "Epoch 16 Batch 700 Loss 0.4323\n",
            "Epoch 16 Loss 0.4463\n",
            "Time taken for 1 epoch 369.8447780609131 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.3920\n",
            "Epoch 17 Batch 100 Loss 0.5043\n",
            "Epoch 17 Batch 200 Loss 0.4711\n",
            "Epoch 17 Batch 300 Loss 0.4528\n",
            "Epoch 17 Batch 400 Loss 0.4423\n",
            "Epoch 17 Batch 500 Loss 0.4579\n",
            "Epoch 17 Batch 600 Loss 0.4477\n",
            "Epoch 17 Batch 700 Loss 0.4618\n",
            "Epoch 17 Loss 0.4557\n",
            "Time taken for 1 epoch 366.09781289100647 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.4377\n",
            "Epoch 18 Batch 100 Loss 0.4333\n",
            "Epoch 18 Batch 200 Loss 0.4356\n",
            "Epoch 18 Batch 300 Loss 0.4391\n",
            "Epoch 18 Batch 400 Loss 0.4215\n",
            "Epoch 18 Batch 500 Loss 0.4180\n",
            "Epoch 18 Batch 600 Loss 0.4317\n",
            "Epoch 18 Batch 700 Loss 0.4274\n",
            "Epoch 18 Loss 0.4296\n",
            "Time taken for 1 epoch 368.95478987693787 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.4258\n",
            "Epoch 19 Batch 100 Loss 0.4162\n",
            "Epoch 19 Batch 200 Loss 0.4060\n",
            "Epoch 19 Batch 300 Loss 0.4271\n",
            "Epoch 19 Batch 400 Loss 0.4053\n",
            "Epoch 19 Batch 500 Loss 0.4170\n",
            "Epoch 19 Batch 600 Loss 0.4216\n",
            "Epoch 19 Batch 700 Loss 0.4297\n",
            "Epoch 19 Loss 0.4108\n",
            "Time taken for 1 epoch 365.554726600647 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.4202\n",
            "Epoch 20 Batch 100 Loss 0.3879\n",
            "Epoch 20 Batch 200 Loss 0.3908\n",
            "Epoch 20 Batch 300 Loss 0.5507\n",
            "Epoch 20 Batch 400 Loss 0.5139\n",
            "Epoch 20 Batch 500 Loss 0.4806\n",
            "Epoch 20 Batch 600 Loss 0.4739\n",
            "Epoch 20 Batch 700 Loss 0.4658\n",
            "Epoch 20 Loss 0.4557\n",
            "Time taken for 1 epoch 367.8493058681488 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.4376\n",
            "Epoch 21 Batch 100 Loss 0.4288\n",
            "Epoch 21 Batch 200 Loss 0.3960\n",
            "Epoch 21 Batch 300 Loss 0.5439\n",
            "Epoch 21 Batch 400 Loss 0.5338\n",
            "Epoch 21 Batch 500 Loss 0.5027\n",
            "Epoch 21 Batch 600 Loss 0.4851\n",
            "Epoch 21 Batch 700 Loss 0.4732\n",
            "Epoch 21 Loss 0.4822\n",
            "Time taken for 1 epoch 365.54971575737 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.4666\n",
            "Epoch 22 Batch 100 Loss 0.4442\n",
            "Epoch 22 Batch 200 Loss 0.4285\n",
            "Epoch 22 Batch 300 Loss 0.4599\n",
            "Epoch 22 Batch 400 Loss 0.3946\n",
            "Epoch 22 Batch 500 Loss 0.3963\n",
            "Epoch 22 Batch 600 Loss 0.3653\n",
            "Epoch 22 Batch 700 Loss 0.4449\n",
            "Epoch 22 Loss 0.4281\n",
            "Time taken for 1 epoch 367.5589940547943 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.4079\n",
            "Epoch 23 Batch 100 Loss 0.3399\n",
            "Epoch 23 Batch 200 Loss 0.3593\n",
            "Epoch 23 Batch 300 Loss 0.3762\n",
            "Epoch 23 Batch 400 Loss 0.3401\n",
            "Epoch 23 Batch 500 Loss 0.3278\n",
            "Epoch 23 Batch 600 Loss 0.3579\n",
            "Epoch 23 Batch 700 Loss 0.3292\n",
            "Epoch 23 Loss 0.3583\n",
            "Time taken for 1 epoch 366.286333322525 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.3050\n",
            "Epoch 24 Batch 100 Loss 0.3400\n",
            "Epoch 24 Batch 200 Loss 0.3018\n",
            "Epoch 24 Batch 300 Loss 0.3135\n",
            "Epoch 24 Batch 400 Loss 0.3386\n",
            "Epoch 24 Batch 500 Loss 0.2613\n",
            "Epoch 24 Batch 600 Loss 0.2976\n",
            "Epoch 24 Batch 700 Loss 0.3046\n",
            "Epoch 24 Loss 0.2977\n",
            "Time taken for 1 epoch 366.87478470802307 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.2370\n",
            "Epoch 25 Batch 100 Loss 0.2546\n",
            "Epoch 25 Batch 200 Loss 0.2743\n",
            "Epoch 25 Batch 300 Loss 0.2433\n",
            "Epoch 25 Batch 400 Loss 0.2794\n",
            "Epoch 25 Batch 500 Loss 0.2442\n",
            "Epoch 25 Batch 600 Loss 0.2293\n",
            "Epoch 25 Batch 700 Loss 0.2566\n",
            "Epoch 25 Loss 0.2634\n",
            "Time taken for 1 epoch 364.71471309661865 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.2462\n",
            "Epoch 26 Batch 100 Loss 0.2031\n",
            "Epoch 26 Batch 200 Loss 0.2253\n",
            "Epoch 26 Batch 300 Loss 0.1953\n",
            "Epoch 26 Batch 400 Loss 0.2148\n",
            "Epoch 26 Batch 500 Loss 0.2375\n",
            "Epoch 26 Batch 600 Loss 0.2539\n",
            "Epoch 26 Batch 700 Loss 0.2352\n",
            "Epoch 26 Loss 0.2264\n",
            "Time taken for 1 epoch 367.66987586021423 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.2376\n",
            "Epoch 27 Batch 100 Loss 0.2006\n",
            "Epoch 27 Batch 200 Loss 0.2087\n",
            "Epoch 27 Batch 300 Loss 0.2061\n",
            "Epoch 27 Batch 400 Loss 0.1883\n",
            "Epoch 27 Batch 500 Loss 0.2069\n",
            "Epoch 27 Batch 600 Loss 0.1980\n",
            "Epoch 27 Batch 700 Loss 0.2510\n",
            "Epoch 27 Loss 0.2124\n",
            "Time taken for 1 epoch 365.7725374698639 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.2975\n",
            "Epoch 28 Batch 100 Loss 0.2027\n",
            "Epoch 28 Batch 200 Loss 0.2515\n",
            "Epoch 28 Batch 300 Loss 0.1997\n",
            "Epoch 28 Batch 400 Loss 0.2125\n",
            "Epoch 28 Batch 500 Loss 0.2065\n",
            "Epoch 28 Batch 600 Loss 0.1787\n",
            "Epoch 28 Batch 700 Loss 0.2244\n",
            "Epoch 28 Loss 0.2218\n",
            "Time taken for 1 epoch 370.47591519355774 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.2396\n",
            "Epoch 29 Batch 100 Loss 0.1837\n",
            "Epoch 29 Batch 200 Loss 0.1847\n",
            "Epoch 29 Batch 300 Loss 0.1727\n",
            "Epoch 29 Batch 400 Loss 0.1586\n",
            "Epoch 29 Batch 500 Loss 0.1735\n",
            "Epoch 29 Batch 600 Loss 0.1287\n",
            "Epoch 29 Batch 700 Loss 0.1732\n",
            "Epoch 29 Loss 0.1713\n",
            "Time taken for 1 epoch 365.81302428245544 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.1222\n",
            "Epoch 30 Batch 100 Loss 0.1401\n",
            "Epoch 30 Batch 200 Loss 0.1601\n",
            "Epoch 30 Batch 300 Loss 0.1492\n",
            "Epoch 30 Batch 400 Loss 0.1572\n",
            "Epoch 30 Batch 500 Loss 0.1406\n",
            "Epoch 30 Batch 600 Loss 0.1740\n",
            "Epoch 30 Batch 700 Loss 0.1467\n",
            "Epoch 30 Loss 0.1551\n",
            "Time taken for 1 epoch 368.63155245780945 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.1416\n",
            "Epoch 31 Batch 100 Loss 0.1307\n",
            "Epoch 31 Batch 200 Loss 0.1163\n",
            "Epoch 31 Batch 300 Loss 0.1403\n",
            "Epoch 31 Batch 400 Loss 0.1379\n",
            "Epoch 31 Batch 500 Loss 0.1471\n",
            "Epoch 31 Batch 600 Loss 0.1125\n",
            "Epoch 31 Batch 700 Loss 0.1330\n",
            "Epoch 31 Loss 0.1389\n",
            "Time taken for 1 epoch 364.62709522247314 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.1102\n",
            "Epoch 32 Batch 100 Loss 0.1235\n",
            "Epoch 32 Batch 200 Loss 0.1104\n",
            "Epoch 32 Batch 300 Loss 0.1171\n",
            "Epoch 32 Batch 400 Loss 0.1498\n",
            "Epoch 32 Batch 500 Loss 0.1113\n",
            "Epoch 32 Batch 600 Loss 0.1068\n",
            "Epoch 32 Batch 700 Loss 0.1298\n",
            "Epoch 32 Loss 0.1245\n",
            "Time taken for 1 epoch 368.7795219421387 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.1257\n",
            "Epoch 33 Batch 100 Loss 0.1086\n",
            "Epoch 33 Batch 200 Loss 0.1110\n",
            "Epoch 33 Batch 300 Loss 0.0995\n",
            "Epoch 33 Batch 400 Loss 0.1335\n",
            "Epoch 33 Batch 500 Loss 0.1098\n",
            "Epoch 33 Batch 600 Loss 0.0759\n",
            "Epoch 33 Batch 700 Loss 0.1054\n",
            "Epoch 33 Loss 0.1181\n",
            "Time taken for 1 epoch 366.53685188293457 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0733\n",
            "Epoch 34 Batch 100 Loss 0.0707\n",
            "Epoch 34 Batch 200 Loss 0.1391\n",
            "Epoch 34 Batch 300 Loss 0.1210\n",
            "Epoch 34 Batch 400 Loss 0.0962\n",
            "Epoch 34 Batch 500 Loss 0.1399\n",
            "Epoch 34 Batch 600 Loss 0.1207\n",
            "Epoch 34 Batch 700 Loss 0.1142\n",
            "Epoch 34 Loss 0.1058\n",
            "Time taken for 1 epoch 370.32293486595154 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0930\n",
            "Epoch 35 Batch 100 Loss 0.0848\n",
            "Epoch 35 Batch 200 Loss 0.1176\n",
            "Epoch 35 Batch 300 Loss 0.1024\n",
            "Epoch 35 Batch 400 Loss 0.1076\n",
            "Epoch 35 Batch 500 Loss 0.0763\n",
            "Epoch 35 Batch 600 Loss 0.0979\n",
            "Epoch 35 Batch 700 Loss 0.1013\n",
            "Epoch 35 Loss 0.1000\n",
            "Time taken for 1 epoch 366.5131573677063 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0640\n",
            "Epoch 36 Batch 100 Loss 0.1055\n",
            "Epoch 36 Batch 200 Loss 0.0823\n",
            "Epoch 36 Batch 300 Loss 0.1113\n",
            "Epoch 36 Batch 400 Loss 0.0972\n",
            "Epoch 36 Batch 500 Loss 0.0894\n",
            "Epoch 36 Batch 600 Loss 0.1212\n",
            "Epoch 36 Batch 700 Loss 0.1111\n",
            "Epoch 36 Loss 0.0908\n",
            "Time taken for 1 epoch 367.7602939605713 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0748\n",
            "Epoch 37 Batch 100 Loss 0.0859\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt6tcDATNf37"
      },
      "source": [
        "## 8: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0SToVxUs4z6"
      },
      "source": [
        "def evaluate(sentence):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to predict result\n",
        "    INPUT: \n",
        "    sentence: input sentence \n",
        "    OUTPUT: \n",
        "    result: predict result\n",
        "    sentence: input sentence \n",
        "    attention_plot: attention weights\n",
        "    ''' \n",
        "    attention_plot = np.zeros((target_max_length, input_max_length))\n",
        "\n",
        "    #sentence = preprocess_sentence(sentence)\n",
        "    sentence = '<start> '+sentence + ' <end>'\n",
        "    \n",
        "    #print(input_tokenizer.texts_to_sequences([i])[0][0])\n",
        "    #print(sentence.split(' '))\n",
        "    inputs = [input_tokenizer.texts_to_sequences([i])[0][0] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=input_max_length,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(target_max_length):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if target_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPoduNy7s6Fm"
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  '''\n",
        "    DESCRIPTION:\n",
        "    This function to plot attention \n",
        "    INPUT: \n",
        "    attention: attention weights\n",
        "    sentence: input sentence \n",
        "    predicted_sentence: predict result\n",
        "\n",
        "    OUTPUT: \n",
        "    None\n",
        "    ''' \n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuwGAmOptAJ5"
      },
      "source": [
        "def predict(sentence):\n",
        "    '''\n",
        "    DESCRIPTION:\n",
        "    This function to predict output sentence\n",
        "\n",
        "    INPUT: \n",
        "    sentence: input sentence \n",
        "\n",
        "    OUTPUT: \n",
        "    None\n",
        "    ''' \n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input:\\n %s' % (sentence))\n",
        "\n",
        "    predict = ''\n",
        "    sentence_list = sentence.split(' ')\n",
        "    sentence_list.pop(0) # remove <start>\n",
        "    result_list = result.split(' ')\n",
        "\n",
        "    for i in range(len(sentence_list)):\n",
        "      if (result_list[i]=='space'):\n",
        "        predict += sentence_list[i]+' '\n",
        "      else:\n",
        "        predict += sentence_list[i]+result_list[i]+' '\n",
        "    print('Predicted punctuation:\\n {}'.format(predict))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence_list, result_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmbqzIQEN16G"
      },
      "source": [
        "### 8.1: Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "RSc2ENEutDST",
        "outputId": "dafa0c4e-17c2-4473-ba99-a5f64eeeb0c3"
      },
      "source": [
        "\n",
        "real_sentence = 'جدد المنتخب المصري فوزه على مضيفه منتخب إي سواتيني 2-صفر خلال الجولة'\n",
        "sentence = 'جدد المنتخب المصري فوزه على مضيفه منتخب إي سواتيني 2-صفر خلال الجولة'\n",
        "print('Original Text: ',real_sentence)\n",
        "\n",
        "in_seq = sentence.strip().split(' ')\n",
        "n =15\n",
        "in_sequances = [\" \".join(in_seq[i:i+n]) for i in range(0, len(in_seq), n)]\n",
        "[predict(s) for s in in_sequances]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Text:  جدد المنتخب المصري فوزه على مضيفه منتخب إي سواتيني 2-صفر خلال الجولة\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-0daf8bfe3ddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0min_sequances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0min_sequances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-0daf8bfe3ddf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0min_sequances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0min_sequances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYxdZ9m6ODkR"
      },
      "source": [
        "### 8.2 Evaluate Model Using BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcVph5m3LVay"
      },
      "source": [
        "def evaluate_model( sentences, target):\n",
        "  '''\n",
        "  DESCRIPTION:\n",
        "  This function to evaluate model \n",
        "  INPUT: \n",
        "  sentences: input vector\n",
        "  target: target vector\n",
        "  OUTPUT: \n",
        "  actual: real target sentences\n",
        "  predicted: predict target sentences\n",
        "  ''' \n",
        "  actual, predicted = list(), list()\n",
        "  outer = tqdm(range(len(sentences)),leave=True,position =0)\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    # translate encoded source text\n",
        "    #sentence = sentence.reshape((1, sentence.shape[0]))\n",
        "    predict,raw_src,_ = evaluate(sentence)\n",
        "    predict = predict.replace(' <end> ','')\n",
        "    raw_target = target[i]\n",
        "    if i < 10:\n",
        "      print('src=[%s], target=[%s], predicted=[%s]' % (sentence, raw_target, predict))\n",
        "    actual.append([raw_target.split()])\n",
        "    predicted.append(predict.split())\n",
        "    outer.update(1)\n",
        "\n",
        "  return actual, predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dvkx1lnLez9",
        "outputId": "07bd0f56-3bb8-4215-fde3-7fcdaff53184"
      },
      "source": [
        "input_ = np.array(source_dataset['sentences'].map(lambda s: ' '.join(s.split())))\n",
        "input_ = [ t.replace(' <end>','') for t in input_]\n",
        "input_ = [ t.replace('<start> ','') for t in input_]\n",
        "\n",
        "target_ = np.array(target_dataset['sentences'].map(lambda s: ' '.join(s.split())))\n",
        "target_ = [ t.replace(' <end>','') for t in target_]\n",
        "target_ = [ t.replace('<start> ','') for t in target_]\n",
        "\n",
        "actual, predicted = evaluate_model(input_,target_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 1/91875 [00:01<30:14:00,  1.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[جدد المنتخب المصري فوزه على مضيفه منتخب إي سواتيني 2 - صفر خلال], target=[space space space space space space space space space ، space space space ، space], predicted=[space space space space space space space space space ، space space space ، space]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 2/91875 [00:02<30:02:11,  1.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[الجولة الرابعة من تصفيات كأس أمم أفريقيا على استاد السلام بالقاهرة الثلاثاء], target=[space space space space space space space ، space space space space ، space .], predicted=[space space space space space space space ، space space space space ، space .]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 3/91875 [00:03<29:58:55,  1.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[وحملت ثنائية الفراعنة توقيع أحمد حجازي ( 19 ) مروان محسن ( 53 )], target=[space space space space space space space space space space space space space space .], predicted=[space space space space space space space space space space space space space space .]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 4/91875 [00:04<30:02:03,  1.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[وبهذه النتيجة رفع منتخب الفراعنة رصيده إلى 9 نقاط في المجموعة 11 من التصفيات], target=[space space space space space space space space space space space space space space ،], predicted=[space space space space space space space space space space space space space space ،]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 5/91875 [00:05<29:54:29,  1.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[والتي تضم إلى جانبه كلا من تونس وإي سواتيني والنيجر وكانت مباراة الذهاب قد], target=[space space space space space space space space space space . space space space space], predicted=[space space space space space space space space space space . space space space space]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 6/91875 [00:07<29:46:37,  1.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[انتهت بفوز رفاق محمد صلاح على إي سواتيني 4 - 1 وشهدت المباراة غياب], target=[space space space space space space space space space space space . space space space], predicted=[space space space space space space space space space space space . space space space]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 7/91875 [00:08<30:02:46,  1.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[نجم ليفربول محمد صلاح بداعي إصابة تعرض لها في مباراة الذهاب وبخسارتها الثالثة], target=[space space space space ، space space space space space space space . space space], predicted=[space space space space ، space space space space space space space . space space]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 8/91875 [00:09<30:05:43,  1.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[في أربع جولات مقابل تعادل واحد أصبحت إي سواتيني خارج حسابات التأهل إلى النهائيات], target=[space space space space space space ، space space space space space space space space], predicted=[space space space space space space ، space space space space space space space space]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 9/91875 [00:10<30:03:08,  1.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[التي يبلغها بطل ووصيف المجموعة وفي حال تعادل تونس أو فوزها لاحقا على النيجر], target=[space space space space space ، space space space space space space space space space], predicted=[space space space space space ، space space space space space space space space space]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 10/91875 [00:11<29:53:19,  1.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "src=[( نقطة واحدة ) ستضمن مع مصر بلوغها النهائيات قبل جولتين على نهاية], target=[space space space space ، space space space space space ، space space space space], predicted=[space space space space ، space space space space space ، space space space space]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  9%|▊         | 7909/91875 [2:38:45<27:03:33,  1.16s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7CO2sPvNZxM"
      },
      "source": [
        "# calculate BLEU score\n",
        "print('BLEU: %f' % corpus_bleu(actual, predicted))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}